{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TwoLayerConv\n",
    "from dataset import CSVLoader, UnNormalize\n",
    "from train import train_model\n",
    "from eval import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User model, loss function, optimizer, and hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "epochs     = 50\n",
    "lr         = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Model, Loss_fn, and Optimizer\n",
    "model      = TwoLayerConv()\n",
    "criterion  = torch.nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.Adam(params = model.parameters(), lr = lr)\n",
    "device     = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mu = [0.49139968, 0.48215827, 0.44653124]\n",
    "cifar_sd = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "tf = transforms.Compose([transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = cifar_mu, std = cifar_sd),\n",
    "                         transforms.RandomHorizontalFlip()\n",
    "                         ])\n",
    "\n",
    "tf_test = transforms.Compose([transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = cifar_mu, std = cifar_sd)\n",
    "                         ])\n",
    "\n",
    "\n",
    "# Downloaded from pytorch\n",
    "dataset = datasets.CIFAR10(root='data/', download = True, transform = tf)\n",
    "test_dataset = datasets.CIFAR10(root='data/', train = False, transform = tf_test)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [45000, 5000])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = 4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_tr = transforms.Compose([UnNormalize(cifar_mu, cifar_sd),\n",
    "                            transforms.ToPILImage()])\n",
    "\n",
    "images = next(iter(train_loader))[0]\n",
    "_, ax = plt.subplots(3, 3)\n",
    "\n",
    "counter = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        \n",
    "        image = im_tr(images[counter])\n",
    "        ax[i, j].imshow(image)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranining Begins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, losses, accuracies, val_losses, val_accuracies = train_model(model, epochs, train_loader,\n",
    " val_loader, optimizer, criterion, device, track_loss = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracies and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label = 'Train')\n",
    "plt.legend()\n",
    "plt.plot(val_accuracies, label = 'Val')\n",
    "plt.legend()\n",
    "plt.title('Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([loss * .10 for loss in losses], label = 'Train')\n",
    "plt.legend()\n",
    "plt.plot([val_loss * .90 for val_loss in val_losses], label = 'Val')\n",
    "plt.legend()\n",
    "plt.title('Losses')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac9ed3ba64d9eb6f56d87b4e348baa285ec687f2194fcec3ab5727c1492a5e15"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('clip': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
